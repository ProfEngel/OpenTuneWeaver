{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "N6vmd9UIUmCl",
        "eUIqjWYEVBf3",
        "l27bEb-jVeNr",
        "uJdvqOdcjUA2",
        "WbQIj0zpkSCl",
        "j-XFl3AllS2E",
        "UOP6-WkblpVO"
      ],
      "authorship_tag": "ABX9TyPZZGZ8e4Jr2SzgXfDcXObK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfEngel/OpenTuneWeaver/blob/main/OpenTuneWeaver_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning eines LLM\n",
        "\n",
        "Mithilfe dieses Colab-Notebooks kann man mit wenigen Schritten ein kleines Sprachmodell auf eigene Anweisungen feinabstimmen. Dafür sind ein paar Vorarbeiten wichtig. Daher bitte Schritt für Schritt dieses Notebook durchgehen und wenn erforderlich Anpassungen vornehmen.\n",
        "\n",
        "Diese Anleitung funktioniert vollständig in der freien colab-Version. Bitte beachten: Für die Laufzeit muss eine GPU gewählt werden. In der freien Version wäre das die T4 NVIDIA GPU. Diese auswählen und starten....\n",
        "\n",
        "Viel Erfolg!"
      ],
      "metadata": {
        "id": "aNEfvHDNTUIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vorbereiten des Q&A-Datensatzes\n",
        "1.   Mit T4-GPU (mindestens) eine Laufzeit starten.\n",
        "2.   Nun das Dokument für das Finetuning wie folgt hochladen: **data.pdf**"
      ],
      "metadata": {
        "id": "8LkfS6aihAT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Einrichtung der Arbeitsumgebung und Start des Ollama-Servers\n",
        "Dieser Codeblock ist dafür zuständig, die Arbeitsumgebung für die Verwendung der Ollama KI-API vorzubereiten und den Ollama-Server zu starten. Hier eine kurze Erklärung der Schritte:\n",
        "\n",
        "* Installation notwendiger Pakete: Zuerst werden einige Linux-Pakete mit apt-get install und das Python-Paket PyPDF2 installiert. PyPDF2 ist eine Bibliothek zur Arbeit mit PDF-Dateien in Python. Das Installieren dieser Pakete ermöglicht die Nutzung von spezifischer Software und Bibliotheken, die für die folgenden Schritte benötigt werden.\n",
        "\n",
        "* Herunterladen und Einrichten von Ollama: Mit dem Befehl curl wird ein Skript von der Ollama-Webseite heruntergeladen und ausgeführt, das für die Installation und Einrichtung der Ollama KI-API sorgt. Dies ermöglicht die Verwendung der Ollama-KI-Funktionalitäten direkt im Notebook.\n",
        "\n",
        "* Starten des Ollama-Servers: Durch die Definition und Ausführung der ollama-Funktion in einem separaten Thread wird der Ollama-Server gestartet. Dieser Server ermöglicht es, Anfragen an die Ollama KI-API zu senden und Antworten zu erhalten. Die Umgebungsvariablen OLLAMA_HOST und OLLAMA_ORIGINS werden gesetzt, um den Server zu konfigurieren.\n",
        "\n",
        "* Herunterladen von KI-Modellen: Der Befehl ollama pull mistral lädt das KI-Modell „mistral“ herunter, das für die Verarbeitung der Anfragen verwendet wird. Die auskommentierten Zeilen zeigen, wie man weitere Modelle herunterladen könnte, diese sind jedoch in diesem Beispiel nicht aktiviert.\n",
        "\n",
        "Insgesamt bereitet dieser Codeblock alles Nötige vor, um den Ollama-Server zu starten und Anfragen an die Ollama KI-API zu senden. Dies ermöglicht es, fortgeschrittene KI-Funktionalitäten wie Textanalyse oder Frage-Antwort-Systeme direkt im Notebook zu nutzen."
      ],
      "metadata": {
        "id": "N6vmd9UIUmCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tkKzx-_sjcDF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! sudo apt-get install -y pciutils\n",
        "!pip install pypdf2\n",
        "! curl https://ollama.ai/install.sh | sh\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "! ollama pull llama3.1\n",
        "#! ollama pull mixtral\n",
        "#! ollama pull llama2:13b\n",
        "\n",
        "#ollama_thread = threading.Thread(target=ollama)\n",
        "#ollama_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Test der KI-Anfrage mit Ollama\n",
        "Dieser Codeblock führt einen Test durch, um zu überprüfen, ob die Einrichtung erfolgreich war und der Ollama-Server richtig funktioniert. Er sendet eine spezifische Frage an die KI und zeigt die Antwort an. Hier sind die Schlüsselschritte erklärt:\n",
        "\n",
        "\n",
        "*   Definieren der Anfrage: Es wird ein Text (prompt) definiert, der die Frage enthält, welche die KI beantworten soll. In diesem Fall geht es um eine kurze Erklärung zum Klimawandel. Dieser Text wird an die KI gesendet, um zu sehen, wie sie komplexe Fragen in einer einfachen und verständlichen Form beantwortet.\n",
        "\n",
        "*  Vorbereitung der Anfrage: Die Anfrage wird als payload vorbereitet, das die Details zur Anfrage, wie das zu verwendende KI-Modell (mixtral), die Temperatur für die Antwortgenerierung (0.6), und die Nachrichten enthält, die den Rahmen für die KI-Antwort setzen. Hier wird die KI als \"hilfreicher KI-Assistent\" angesprochen, um den Kontext für die Antwort zu geben.\n",
        "\n",
        "*  Senden der Anfrage: Die Anfrage wird mithilfe der requests.post-Funktion an die Ollama-API gesendet. Die API-Adresse (url) verweist auf den lokal gestarteten Ollama-Server.\n",
        "\n",
        "*  Anzeigen der Antwort: Nach dem Empfang der Antwort vom Server wird diese von einem Byte-String in ein JSON-Objekt umgewandelt (json.loads). Die Antwort der KI auf die gestellte Frage wird dann auf der Konsole ausgegeben.\n",
        "\n",
        "Dieser Test ist ein wichtiger Schritt, um sicherzustellen, dass alles korrekt eingerichtet ist und die KI wie erwartet auf Anfragen reagiert. Es hilft dabei, eventuelle Probleme in der Einrichtung oder Kommunikation mit dem Ollama-Server frühzeitig zu erkennen und zu beheben."
      ],
      "metadata": {
        "id": "eUIqjWYEVBf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Was bedeutet der Klimawandel?\n",
        "Kannst Du das in drei Sätzen erklären!\n",
        "\"\"\"\n",
        "\n",
        "url = 'http://localhost:11434/api/chat'\n",
        "payload = {\n",
        "    \"model\": \"llama3.1\",\n",
        "    \"temperature\": 0.6,\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher KI-Assistent!\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "message_str = response.content.decode('utf-8')\n",
        "message_dict = json.loads(message_str)\n",
        "print(message_dict['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trqG-h_uj-eZ",
        "outputId": "054c621a-ea93-4da0-a461-d9280d39b0bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Der Klimawandel bezeichnet die Zunahme der durchschnittlichen Temperaturen auf der Erde über einen längeren Zeitraum, verursacht hauptsächlich durch den Anstieg von Kohlendioxid (CO2) und anderen Treibhausgasen in der Atmosphäre. Dieser Prozess führt zu einer globale Erwärmung, die weitreichige Auswirkungen auf das Klima und Ökosysteme hat, einschließlich steigender Meeresspiegel, vermehrter extreme Wetterereignisse und veränderten Niederschlagsmuster.\n",
            "\n",
            "Der Klimawandel ist durch eine Reihe von Faktoren verursacht, darunter die Verbrennung fossiler Brennstoffe wie Kohle, Öl und Gas, die Abholzung von Wäldern sowie die Landwirtschaft. Die Folgen des Klimawandels sind vielfältig und reichen von der Zerstörung von Lebensräumen bis hin zu den Auswirkungen auf die menschliche Gesundheit und Wirtschaft.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Vorbereitung und Analyse eines PDF-Dokuments für Feinabstimmung\n",
        "Dieser Codeblock leitet einen Prozess zur Vorbereitung eines PDF-Dokuments für die Feinabstimmung mit einer KI ein, indem er Fragen und Antworten basierend auf dem Inhalt des Dokuments generiert. Der Prozess ist in mehrere Schlüsselschritte unterteilt:\n",
        "\n",
        "*   Text aus PDF extrahieren: Zunächst wird der Text aus der hochgeladenen PDF-Datei (data.pdf) extrahiert. Benutzer sollten darauf achten, eine PDF mit dem Namen data.pdf hochzuladen. Der Text wird dann für die weitere Verarbeitung gespeichert.\n",
        "\n",
        "*   Text in Abschnitte unterteilen: Um die Bearbeitung zu vereinfachen und die Belastung für die KI zu reduzieren, wird der Text in kleinere Abschnitte unterteilt. Diese Aufteilung hilft, den Text besser handhabbar zu machen und optimiert den Prozess der Frage-Antwort-Erzeugung.\n",
        "\n",
        "*   Frage-Antwort-Paare erzeugen: Für jeden Textabschnitt werden spezifische Frage-Antwort-Paare generiert. Dies geschieht durch das Senden des Textes an die Ollama KI-API, die instruiert wird, präzise und relevante Fragen und Antworten basierend auf dem Textinhalt zu formulieren.\n",
        "\n",
        "*   Ausgabe der aktuellen Q&A: Während des Prozesses wird jede generierte Frage-Antwort-Paarung ausgegeben. Dies ermöglicht eine sofortige Überprüfung und gewährleistet, dass die Ergebnisse wie erwartet sind. Es bietet auch die Möglichkeit, den Fortschritt zu verfolgen.\n",
        "\n",
        "*   Speichern der Ergebnisse: Am Ende werden alle generierten Frage-Antwort-Paare in einer Datei (data_qa.json) gespeichert. Diese Datei kann heruntergeladen, überarbeitet und für die Feinabstimmung der KI oder andere Zwecke verwendet werden.\n",
        "\n",
        "Dieser Prozess ist besonders nützlich für Anwender, die ihre KI-Modelle mit spezifischem Wissen aus Dokumenten anreichern oder trainieren möchten. Die generierten Q&A-Paare dienen als Basis für das Feintuning, wodurch die KI in der Lage ist, auf ähnliche Fragen in Zukunft genauer und kontextbezogener zu antworten."
      ],
      "metadata": {
        "id": "l27bEb-jVeNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import json\n",
        "import requests\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    pdf_file_obj = open(file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page_obj = pdf_reader.pages[page_num]\n",
        "        text += page_obj.extract_text()\n",
        "    pdf_file_obj.close()\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=512):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def submit_to_ollama_api(prompt, retries=3):\n",
        "    url = 'http://localhost:11434/api/chat'\n",
        "    detailed_instruction = (\n",
        "        \"Basierend auf dem folgenden Text, formuliere bitte klare und präzise Frage-Antwort-Paare. \"\n",
        "        \"Die Fragen sollten mit 'Im Kontext ...' beginnen, um den Bezug zum Text deutlich zu machen. \"\n",
        "        \"Antworten sollten direkt aus dem Text abgeleitet und wortwörtlich übernommen werden. \"\n",
        "        \"Ziel ist es, die Informationen im Text durch diese Q&A zugänglich zu machen. \"\n",
        "        \"Hier ist ein Beispiel, wie deine Antworten aussehen sollten:\\n\\n\"\n",
        "        \"Hier zunächst der Beispieltext aus dem Q&A erzeugt werden:\\n\"\n",
        "        \"Erneuerbare Energien wie Solar- und Windkraft werden immer wichtiger, um den Klimawandel zu bekämpfen. Im Gegensatz zu fossilen Brennstoffen, die begrenzt sind, bieten erneuerbare Energien eine nachhaltige Alternative, die die Umwelt schont. Zudem sind die Kosten für die Erzeugung von Strom aus erneuerbaren Quellen in den letzten Jahren stark gesunken, was zu einer höheren Akzeptanz und Verbreitung geführt hat.\\n\"\n",
        "        \"Hier ein paar Beispiele für Frage-Antwort-Paare:\\n\"\n",
        "        \"Frage: Im Kontext der erneuerbaren Energien, warum werden Solar- und Windkraft immer wichtiger?\\n\"\n",
        "        \"Antwort: Erneuerbare Energien wie Solar- und Windkraft werden immer wichtiger, um den Klimawandel zu bekämpfen.\\n\"\n",
        "        \"Frage: Im Kontext der Energiequellen, was ist der Vorteil von erneuerbaren Energien im Vergleich zu fossilen Brennstoffen?\\n\"\n",
        "        \"Antwort: Der Vorteil von erneuerbaren Energien ist, dass sie eine nachhaltige Alternative bieten, im Gegensatz zu fossilen Brennstoffen, die begrenzt sind. Während fossile Brennstoffe bei der Verbrennung CO₂ freisetzen, das für den Klimawandel verantwortlich ist, sind erneuerbare Energien emissionsfrei. Darüber hinaus schont ihre Nutzung die Umwelt, und sie sind unerschöpflich verfügbar, was ihre Bedeutung langfristig noch erhöht.\\n\"\n",
        "        \"Frage: Im Kontext der erneuerbaren Energien, was hat zur höheren Akzeptanz von erneuerbaren Energien geführt?\\n\"\n",
        "        \"Antwort: Die Kosten für die Erzeugung von Strom aus erneuerbaren Quellen sind in den letzten Jahren stark gesunken. Dieser Preisrückgang hat erneuerbare Energien wettbewerbsfähiger gemacht und ihre Akzeptanz sowohl bei Konsumenten als auch bei Unternehmen erhöht. Zusätzlich haben staatliche Förderprogramme und Subventionen dazu beigetragen, dass erneuerbare Energien eine zentrale Rolle in der Energieversorgung einnehmen. Die Kombination aus Kostensenkungen und politischen Maßnahmen hat entscheidend zur Verbreitung von erneuerbaren Energien beigetragen.\\n\"\n",
        "    )\n",
        "    payload = {\n",
        "        \"model\": \"llama3.1\",\n",
        "        \"temperature\": 0.5,\n",
        "        \"stream\": False,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": detailed_instruction},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            if response.status_code == 200:\n",
        "                message_str = response.content.decode('utf-8')\n",
        "                message_dict = json.loads(message_str)\n",
        "                if 'content' in message_dict.get('message', {}):\n",
        "                    print(\"Aktuelle Q&A:\")\n",
        "                    print(message_dict['message']['content'])\n",
        "                    print(\"-\" * 80)  # Eine Trennlinie für bessere Lesbarkeit\n",
        "                    return message_dict['message']['content']\n",
        "                else:\n",
        "                    print(\"No valid content found in the response.\")\n",
        "            else:\n",
        "                print(f\"Attempt {i + 1} failed with status code {response.status_code}. Retrying...\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "    print(\"Max retries exceeded. Skipping this prompt.\")\n",
        "    return None\n",
        "\n",
        "text = extract_text_from_pdf('data.pdf')\n",
        "chunks = split_text_into_chunks(text, 512)\n",
        "\n",
        "responses = []\n",
        "for chunk in chunks:\n",
        "    response = submit_to_ollama_api(chunk)\n",
        "    if response:\n",
        "        responses.append(response)\n",
        "        print(\"Aktuelle Q&A:\")\n",
        "        print(response)\n",
        "        print(\"-\" * 80)  # Eine Trennlinie für bessere Lesbarkeit\n",
        "\n",
        "# Optional: Speichern der Antworten in einer Datei oder weitere Verarbeitung\n",
        "with open('data_qa.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(responses, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "AnTHkrE2OwqU",
        "outputId": "b5407183-9264-4d3c-b1a9-8cd368c10709"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 28) (<ipython-input-5-b24be78c740f>, line 28)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-b24be78c740f>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    \"Hier ein paar Beispiele für Frage-Antwort-Paare:\\\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# WICHTIG!!!! Zwischenschritt\n",
        "\n",
        "1.   Q&A-Datensatz herunterladen (data_qa.json)\n",
        "2.   Datensatz überarbeiten (fehlerhafte Elemente löschen/anpassen, bereinigen)\n",
        "3.   Notebook zurücksetzen\n",
        "4.   Datensatz hochladen\n",
        "5.   Notebook neu starten. Wieder mit T4-GPU\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "x00alEtYXN8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Finetuning\n",
        "\n",
        "Das folgende Skript geht zurück auf [Unsloth](https://github.com/unslothai/unsloth). Einem aktuellen Framework um auch mit wenigen Ressourcen schnell qualitativ hochwertige Ergebnisse zu erzielen.\n",
        "\n",
        "Hier der Link zum Original-Notebook von [Mistral7b-Unsloth](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)"
      ],
      "metadata": {
        "id": "S1Uow-0xWFJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Installation nötiger Bibs und Bezug des Modells\n",
        "\n",
        "\n",
        "\n",
        "*  Zunächst müssen relevante Python-Bibs bezogen werden.\n",
        "*  Das Modell Mistral-7b wird heruntergeladen und in 4bit bereitgestellt.\n",
        "*  Das Lora-Adapter-Template wird heruntergeladen um für das Finetuning trainiert zu werden.\n",
        "*  Der Datensatz wird geladen und in das passende Format übertragen"
      ],
      "metadata": {
        "id": "uJdvqOdcjUA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "id": "20EHnQv_W_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "BtBh7H0hXFrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "pobd-hTNX7iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Laden des lokalen Datensatzes\n",
        "with open('data_qa.json', 'r', encoding='utf-8') as file:\n",
        "    data_qa = json.load(file)\n",
        "\n",
        "EOS_TOKEN = \"[EOS]\"  # Beispiel für ein EOS-Token, anpassen nach Bedarf\n",
        "\n",
        "def formatting_prompts_func(data_qa):\n",
        "    texts = []\n",
        "    for qa_pair in data_qa:\n",
        "        # Direkte Verwendung des QA-Paares als \"instruction\" und \"output\", \"input\" bleibt leer\n",
        "        instruction = \"Bitte umformulieren der folgenden Frage und Antwort in ein klares Format.\"\n",
        "        input = \"\"  # Kein spezifischer Eingabetext, da der QA-Text bereits die Information enthält\n",
        "        output = qa_pair  # Direkte Verwendung des QA-Paares aus dem Datensatz\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\" + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# Verarbeitung und Formatierung des Datensatzes\n",
        "formatted_data = formatting_prompts_func(data_qa)\n",
        "\n",
        "# Beispielweise Ausgabe eines formatierten Elements zur Überprüfung\n",
        "print(formatted_data[0])\n"
      ],
      "metadata": {
        "id": "aQ63ZCV5X8yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Angenommen, formatted_data ist eine Liste von formatierten Texten\n",
        "# Beispiel für formatted_data:\n",
        "# formatted_data = [\"### Instruction:\\nFrage ... Antwort ... [EOS]\", \"...\"]\n",
        "\n",
        "# Konvertieren von formatted_data in ein Dataset-Objekt\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n"
      ],
      "metadata": {
        "id": "nHZ1jBE5Zuu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Training\n",
        "\n",
        "(das Training kann 10-30 Minuten dauern, je nach Größe des Datensatzes auch länger)\n",
        "\n",
        "*  Hyperparameter für das Training (können angepasst werden um das Training zu optimieren)\n",
        "*  Darstellen der Ressourcen vor und nach dem Training\n",
        "*  Trainingsverlauf (hier den angegebenen Wert beachten. Dies ist der Verlustwert und er sollte gegen 0 gehen. Sollte dieser steigen, so wäre es nachteilig für das Training)"
      ],
      "metadata": {
        "id": "WbQIj0zpkSCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Konvertieren von formatted_data in ein Dataset-Objekt (vorausgesetzt, formatted_data ist eine Liste von Strings)\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "# Anschließende Anpassungen und Initialisierung des Trainers\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,  # Verwenden Sie das konvertierte Dataset-Objekt hier\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=128,  # Stellen Sie sicher, dass diese Zahl geeignet ist für Ihre Daten\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "feWUHsO2Zet1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ck1Odi8daGwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "oq9AjAfeaJKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats (optional)\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "6qm_TlFZaM8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Speichern des Modells und des Adapters\n",
        "\n",
        "(dies kann bis 20 Minuten dauern)\n",
        "\n",
        "*  Zunächst wird der trainierte Adapter gespeichert\n",
        "*  Nun wird das Modell im 4b-quantisierten Format *.gguf gespeichert"
      ],
      "metadata": {
        "id": "1qCC8cl7kwSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# Save to q4_k_m GGUF\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "metadata": {
        "id": "rQ4H1v41bUdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# WICHTIG!!!! Zwischenschritt\n",
        "\n",
        "1.   model-unsloth.Q4_K_M.gguf herunterladen\n",
        "2.   Notebook mit T4-GPU neu starten\n",
        "3.   model-unsloth.Q4_K_M.gguf hochladen\n",
        "4.   Ab hier weitermachen!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "57Fr7Hi2gNBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Inferenz (Test)"
      ],
      "metadata": {
        "id": "5X0F26PTa8f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Testvorbereitungen\n",
        "\n",
        "\n",
        "\n",
        "*  Instalation aller relevanten Python-Bibs für Ollama\n",
        "*  Erstellen eines Ollama-Modells basierend auf dem trainierten *.gguf-Modells"
      ],
      "metadata": {
        "id": "j-XFl3AllS2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm model-unsloth.F16.gguf\n",
        "!rm -r model\n",
        "!rm -r lora_model\n",
        "!rm -r llama.cpp\n",
        "!rm -r outputs"
      ],
      "metadata": {
        "id": "NnVmDud_ul6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! sudo apt-get install -y pciutils\n",
        "!pip install pypdf2\n",
        "! curl https://ollama.ai/install.sh | sh\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "u0mqg5aRfYwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Dateiname definieren\n",
        "filename = 'Modelfile'\n",
        "\n",
        "# Inhalt der Datei\n",
        "content = 'FROM ./model-unsloth.Q4_K_M.gguf'\n",
        "\n",
        "# Datei erstellen und schreiben\n",
        "with open(filename, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(f\"Datei '{filename}' wurde erfolgreich erstellt.\")\n",
        "\n",
        "!ollama create ft_mistral -f Modelfile\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Befehl definieren\n",
        "command = 'ollama create ft_mistral -f Modelfile'\n",
        "\n",
        "# Befehl ausführen\n",
        "process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Ausgabe und Fehlerausgabe (falls vorhanden) anzeigen\n",
        "print(process.stdout.decode())\n",
        "if process.stderr:\n",
        "    print(\"Fehler:\", process.stderr.decode())\n"
      ],
      "metadata": {
        "id": "6Ql-ynn0bJap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Testdurchführung mit eigenen Prompts\n",
        "\n",
        "\n",
        "\n",
        "*  Eigenen Prompt eingeben um das Modell zu testen"
      ],
      "metadata": {
        "id": "UOP6-WkblpVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benutzeraufforderung für den gesamten Prompt\n",
        "prompt = input(\"Bitte geben Sie Ihren Prompt ein: \")\n",
        "\n",
        "# Ausgabe des Prompts zur Überprüfung\n",
        "print(\"Ihr eingegebener Prompt lautet:\")\n",
        "print(prompt)\n",
        "\n",
        "url = 'http://localhost:11434/api/chat'\n",
        "payload = {\n",
        "    \"model\": \"ft_mistral\",\n",
        "    \"temperature\": 0.6,\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher KI-Assistent!\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "message_str = response.content.decode('utf-8')\n",
        "message_dict = json.loads(message_str)\n",
        "print(message_dict['message']['content'])"
      ],
      "metadata": {
        "id": "_cMZMu3jfghn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}