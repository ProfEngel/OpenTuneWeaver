{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "N6vmd9UIUmCl",
        "eUIqjWYEVBf3",
        "l27bEb-jVeNr",
        "uJdvqOdcjUA2",
        "WbQIj0zpkSCl",
        "j-XFl3AllS2E",
        "UOP6-WkblpVO"
      ],
      "authorship_tag": "ABX9TyMASm4zyrCHwPUUkpQzRty0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2689120816b414c94e0c45b6d160140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_226fb0ff1c0448f5bca72b26f59f66ed",
              "IPY_MODEL_6a3ec42391844aabaa502a87ebfc6441",
              "IPY_MODEL_9b474272a06447479953a5a524e5fd2c"
            ],
            "layout": "IPY_MODEL_4de4193142ba477ab0326cb0cc6af469"
          }
        },
        "226fb0ff1c0448f5bca72b26f59f66ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6536f96c4c5481c8b124a407f81156d",
            "placeholder": "​",
            "style": "IPY_MODEL_e720a359ab504c83857d4dd3a0f6839d",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "6a3ec42391844aabaa502a87ebfc6441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db89ff6a63ec4446b35323a4e382a32b",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6171f4d62a264c9dbe7c34ef19094b1e",
            "value": 6
          }
        },
        "9b474272a06447479953a5a524e5fd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b8c83be0994ffc9d1e0cb7adcd27e7",
            "placeholder": "​",
            "style": "IPY_MODEL_f3a343325f93456eb282612197c98d97",
            "value": " 6/6 [00:03&lt;00:00,  2.23 examples/s]"
          }
        },
        "4de4193142ba477ab0326cb0cc6af469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6536f96c4c5481c8b124a407f81156d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e720a359ab504c83857d4dd3a0f6839d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db89ff6a63ec4446b35323a4e382a32b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6171f4d62a264c9dbe7c34ef19094b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7b8c83be0994ffc9d1e0cb7adcd27e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a343325f93456eb282612197c98d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfEngel/OpenTuneWeaver/blob/main/OpenTuneWeaver_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning eines LLM\n",
        "\n",
        "Mithilfe dieses Colab-Notebooks kann man mit wenigen Schritten ein kleines Sprachmodell auf eigene Anweisungen feinabstimmen. Dafür sind ein paar Vorarbeiten wichtig. Daher bitte Schritt für Schritt dieses Notebook durchgehen und wenn erforderlich Anpassungen vornehmen.\n",
        "\n",
        "Diese Anleitung funktioniert vollständig in der freien colab-Version. Bitte beachten: Für die Laufzeit muss eine GPU gewählt werden. In der freien Version wäre das die T4 NVIDIA GPU. Diese auswählen und starten....\n",
        "\n",
        "Viel Erfolg!"
      ],
      "metadata": {
        "id": "aNEfvHDNTUIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vorbereiten des Q&A-Datensatzes\n",
        "1.   Mit T4-GPU (mindestens) eine Laufzeit starten.\n",
        "2.   Nun das Dokument für das Finetuning wie folgt hochladen: **data.pdf**"
      ],
      "metadata": {
        "id": "8LkfS6aihAT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Einrichtung der Arbeitsumgebung und Start des Ollama-Servers\n",
        "Dieser Codeblock ist dafür zuständig, die Arbeitsumgebung für die Verwendung der Ollama KI-API vorzubereiten und den Ollama-Server zu starten. Hier eine kurze Erklärung der Schritte:\n",
        "\n",
        "* Installation notwendiger Pakete: Zuerst werden einige Linux-Pakete mit apt-get install und das Python-Paket PyPDF2 installiert. PyPDF2 ist eine Bibliothek zur Arbeit mit PDF-Dateien in Python. Das Installieren dieser Pakete ermöglicht die Nutzung von spezifischer Software und Bibliotheken, die für die folgenden Schritte benötigt werden.\n",
        "\n",
        "* Herunterladen und Einrichten von Ollama: Mit dem Befehl curl wird ein Skript von der Ollama-Webseite heruntergeladen und ausgeführt, das für die Installation und Einrichtung der Ollama KI-API sorgt. Dies ermöglicht die Verwendung der Ollama-KI-Funktionalitäten direkt im Notebook.\n",
        "\n",
        "* Starten des Ollama-Servers: Durch die Definition und Ausführung der ollama-Funktion in einem separaten Thread wird der Ollama-Server gestartet. Dieser Server ermöglicht es, Anfragen an die Ollama KI-API zu senden und Antworten zu erhalten. Die Umgebungsvariablen OLLAMA_HOST und OLLAMA_ORIGINS werden gesetzt, um den Server zu konfigurieren.\n",
        "\n",
        "* Herunterladen von KI-Modellen: Der Befehl ollama pull mistral lädt das KI-Modell „mistral“ herunter, das für die Verarbeitung der Anfragen verwendet wird. Die auskommentierten Zeilen zeigen, wie man weitere Modelle herunterladen könnte, diese sind jedoch in diesem Beispiel nicht aktiviert.\n",
        "\n",
        "Insgesamt bereitet dieser Codeblock alles Nötige vor, um den Ollama-Server zu starten und Anfragen an die Ollama KI-API zu senden. Dies ermöglicht es, fortgeschrittene KI-Funktionalitäten wie Textanalyse oder Frage-Antwort-Systeme direkt im Notebook zu nutzen."
      ],
      "metadata": {
        "id": "N6vmd9UIUmCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tkKzx-_sjcDF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! sudo apt-get install -y pciutils\n",
        "!pip install pypdf2\n",
        "! curl https://ollama.ai/install.sh | sh\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "! ollama pull mistral-small:latest\n",
        "#! ollama pull mixtral\n",
        "#! ollama pull llama2:13b\n",
        "\n",
        "#ollama_thread = threading.Thread(target=ollama)\n",
        "#ollama_thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Test der KI-Anfrage mit Ollama\n",
        "Dieser Codeblock führt einen Test durch, um zu überprüfen, ob die Einrichtung erfolgreich war und der Ollama-Server richtig funktioniert. Er sendet eine spezifische Frage an die KI und zeigt die Antwort an. Hier sind die Schlüsselschritte erklärt:\n",
        "\n",
        "\n",
        "*   Definieren der Anfrage: Es wird ein Text (prompt) definiert, der die Frage enthält, welche die KI beantworten soll. In diesem Fall geht es um eine kurze Erklärung zum Klimawandel. Dieser Text wird an die KI gesendet, um zu sehen, wie sie komplexe Fragen in einer einfachen und verständlichen Form beantwortet.\n",
        "\n",
        "*  Vorbereitung der Anfrage: Die Anfrage wird als payload vorbereitet, das die Details zur Anfrage, wie das zu verwendende KI-Modell (mixtral), die Temperatur für die Antwortgenerierung (0.6), und die Nachrichten enthält, die den Rahmen für die KI-Antwort setzen. Hier wird die KI als \"hilfreicher KI-Assistent\" angesprochen, um den Kontext für die Antwort zu geben.\n",
        "\n",
        "*  Senden der Anfrage: Die Anfrage wird mithilfe der requests.post-Funktion an die Ollama-API gesendet. Die API-Adresse (url) verweist auf den lokal gestarteten Ollama-Server.\n",
        "\n",
        "*  Anzeigen der Antwort: Nach dem Empfang der Antwort vom Server wird diese von einem Byte-String in ein JSON-Objekt umgewandelt (json.loads). Die Antwort der KI auf die gestellte Frage wird dann auf der Konsole ausgegeben.\n",
        "\n",
        "Dieser Test ist ein wichtiger Schritt, um sicherzustellen, dass alles korrekt eingerichtet ist und die KI wie erwartet auf Anfragen reagiert. Es hilft dabei, eventuelle Probleme in der Einrichtung oder Kommunikation mit dem Ollama-Server frühzeitig zu erkennen und zu beheben."
      ],
      "metadata": {
        "id": "eUIqjWYEVBf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Was bedeutet der Klimawandel?\n",
        "Kannst Du das in drei Sätzen erklären!\n",
        "\"\"\"\n",
        "\n",
        "url = 'http://localhost:11434/api/chat'\n",
        "payload = {\n",
        "    \"model\": \"mistral-small:latest\",\n",
        "    \"temperature\": 0.6,\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher KI-Assistent!\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "message_str = response.content.decode('utf-8')\n",
        "message_dict = json.loads(message_str)\n",
        "print(message_dict['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trqG-h_uj-eZ",
        "outputId": "5e803dad-bd97-4bdc-d544-39bde3353b86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Der Klimawandel beschreibt die langfristige Veränderung des globalen Klimas, insbesondere durch den Anstieg der Durchschnittstemperaturen. Diese Erwärmung wird hauptsächlich durch menschliche Aktivitäten wie die Verbrennung fossiler Brennstoffe und Abholzung verursacht, was zu einem Anstieg von Treibhausgasen in der Atmosphäre führt. Die Folgen umfassen extreme Wetterereignisse, Meeresspiegelanstieg und Bedrohungen für die Biodiversität.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Vorbereitung und Analyse eines PDF-Dokuments für Feinabstimmung\n",
        "Dieser Codeblock leitet einen Prozess zur Vorbereitung eines PDF-Dokuments für die Feinabstimmung mit einer KI ein, indem er Fragen und Antworten basierend auf dem Inhalt des Dokuments generiert. Der Prozess ist in mehrere Schlüsselschritte unterteilt:\n",
        "\n",
        "*   Text aus PDF extrahieren: Zunächst wird der Text aus der hochgeladenen PDF-Datei (data.pdf) extrahiert. Benutzer sollten darauf achten, eine PDF mit dem Namen data.pdf hochzuladen. Der Text wird dann für die weitere Verarbeitung gespeichert.\n",
        "\n",
        "*   Text in Abschnitte unterteilen: Um die Bearbeitung zu vereinfachen und die Belastung für die KI zu reduzieren, wird der Text in kleinere Abschnitte unterteilt. Diese Aufteilung hilft, den Text besser handhabbar zu machen und optimiert den Prozess der Frage-Antwort-Erzeugung.\n",
        "\n",
        "*   Frage-Antwort-Paare erzeugen: Für jeden Textabschnitt werden spezifische Frage-Antwort-Paare generiert. Dies geschieht durch das Senden des Textes an die Ollama KI-API, die instruiert wird, präzise und relevante Fragen und Antworten basierend auf dem Textinhalt zu formulieren.\n",
        "\n",
        "*   Ausgabe der aktuellen Q&A: Während des Prozesses wird jede generierte Frage-Antwort-Paarung ausgegeben. Dies ermöglicht eine sofortige Überprüfung und gewährleistet, dass die Ergebnisse wie erwartet sind. Es bietet auch die Möglichkeit, den Fortschritt zu verfolgen.\n",
        "\n",
        "*   Speichern der Ergebnisse: Am Ende werden alle generierten Frage-Antwort-Paare in einer Datei (data_qa.json) gespeichert. Diese Datei kann heruntergeladen, überarbeitet und für die Feinabstimmung der KI oder andere Zwecke verwendet werden.\n",
        "\n",
        "Dieser Prozess ist besonders nützlich für Anwender, die ihre KI-Modelle mit spezifischem Wissen aus Dokumenten anreichern oder trainieren möchten. Die generierten Q&A-Paare dienen als Basis für das Feintuning, wodurch die KI in der Lage ist, auf ähnliche Fragen in Zukunft genauer und kontextbezogener zu antworten."
      ],
      "metadata": {
        "id": "l27bEb-jVeNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import json\n",
        "import requests\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    pdf_file_obj = open(file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "    text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page_obj = pdf_reader.pages[page_num]\n",
        "        text += page_obj.extract_text()\n",
        "    pdf_file_obj.close()\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=512):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def submit_to_ollama_api(prompt, retries=3):\n",
        "    url = 'http://localhost:11434/api/chat'\n",
        "    detailed_instruction = (\n",
        "        \"Basierend auf dem folgenden Text, formuliere bitte klare und präzise Frage-Antwort-Paare. Die Frage-Antwort-Paare haben zu 20% nur einen Satz und zu 60% mindestens zwei Sätze und zu 20% mehr als zwei Sätze.\"\n",
        "        \"Die Fragen sollten mit 'Im Kontext ...' beginnen, um den Bezug zum Text deutlich zu machen. \"\n",
        "        \"Antworten sollten direkt aus dem Text abgeleitet und wortwörtlich übernommen werden. \"\n",
        "        \"Ziel ist es, die Informationen im Text durch diese Q&A zugänglich zu machen. \"\n",
        "        \"Hier ist ein Beispiel, wie deine Antworten aussehen sollten:\\n\\n\"\n",
        "        \"Hier zunächst der Beispieltext aus dem Q&A erzeugt werden:\\n\"\n",
        "        \"Erneuerbare Energien wie Solar- und Windkraft werden immer wichtiger, um den Klimawandel zu bekämpfen. Im Gegensatz zu fossilen Brennstoffen, die begrenzt sind, bieten erneuerbare Energien eine nachhaltige Alternative, die die Umwelt schont. Zudem sind die Kosten für die Erzeugung von Strom aus erneuerbaren Quellen in den letzten Jahren stark gesunken, was zu einer höheren Akzeptanz und Verbreitung geführt hat.\\n\"\n",
        "        \"Hier ein paar Beispiele für Frage-Antwort-Paare:\\n\"\n",
        "        \"Frage: Im Kontext der erneuerbaren Energien, warum werden Solar- und Windkraft immer wichtiger?\\n\"\n",
        "        \"Antwort: Erneuerbare Energien wie Solar- und Windkraft werden immer wichtiger, um den Klimawandel zu bekämpfen.\\n\"\n",
        "        \"Frage: Im Kontext der Energiequellen, was ist der Vorteil von erneuerbaren Energien im Vergleich zu fossilen Brennstoffen?\\n\"\n",
        "        \"Antwort: Der Vorteil von erneuerbaren Energien ist, dass sie eine nachhaltige Alternative bieten, im Gegensatz zu fossilen Brennstoffen, die begrenzt sind. Während fossile Brennstoffe bei der Verbrennung CO₂ freisetzen, das für den Klimawandel verantwortlich ist, sind erneuerbare Energien emissionsfrei. Darüber hinaus schont ihre Nutzung die Umwelt, und sie sind unerschöpflich verfügbar, was ihre Bedeutung langfristig noch erhöht.\\n\"\n",
        "        \"Frage: Im Kontext der erneuerbaren Energien, was hat zur höheren Akzeptanz von erneuerbaren Energien geführt?\\n\"\n",
        "        \"Antwort: Die Kosten für die Erzeugung von Strom aus erneuerbaren Quellen sind in den letzten Jahren stark gesunken. Dieser Preisrückgang hat erneuerbare Energien wettbewerbsfähiger gemacht und ihre Akzeptanz sowohl bei Konsumenten als auch bei Unternehmen erhöht. Zusätzlich haben staatliche Förderprogramme und Subventionen dazu beigetragen, dass erneuerbare Energien eine zentrale Rolle in der Energieversorgung einnehmen. Die Kombination aus Kostensenkungen und politischen Maßnahmen hat entscheidend zur Verbreitung von erneuerbaren Energien beigetragen.\\n\"\n",
        "    )\n",
        "    payload = {\n",
        "        \"model\": \"mistral-small:latest\",\n",
        "        \"temperature\": 0.8,\n",
        "        \"stream\": False,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": detailed_instruction},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            if response.status_code == 200:\n",
        "                message_str = response.content.decode('utf-8')\n",
        "                message_dict = json.loads(message_str)\n",
        "                if 'content' in message_dict.get('message', {}):\n",
        "                    print(\"Aktuelle Q&A:\")\n",
        "                    print(message_dict['message']['content'])\n",
        "                    print(\"-\" * 80)  # Eine Trennlinie für bessere Lesbarkeit\n",
        "                    return message_dict['message']['content']\n",
        "                else:\n",
        "                    print(\"No valid content found in the response.\")\n",
        "            else:\n",
        "                print(f\"Attempt {i + 1} failed with status code {response.status_code}. Retrying...\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "    print(\"Max retries exceeded. Skipping this prompt.\")\n",
        "    return None\n",
        "\n",
        "text = extract_text_from_pdf('data.pdf')\n",
        "chunks = split_text_into_chunks(text, 512)\n",
        "\n",
        "responses = []\n",
        "for chunk in chunks:\n",
        "    response = submit_to_ollama_api(chunk)\n",
        "    if response:\n",
        "        responses.append(response)\n",
        "        print(\"Aktuelle Q&A:\")\n",
        "        print(response)\n",
        "        print(\"-\" * 80)  # Eine Trennlinie für bessere Lesbarkeit\n",
        "\n",
        "# Optional: Speichern der Antworten in einer Datei oder weitere Verarbeitung\n",
        "with open('data_qa.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(responses, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnTHkrE2OwqU",
        "outputId": "c1f0ed4c-b91c-4d07-9644-2f6b57272601"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem Text über das Spassball Spiel:\n",
            "\n",
            "Frage: Im Kontext des Spielfelds, welche Besonderheiten gibt es?\n",
            "Antwort: Das Spielfeld ist oval mit zwei Trampolinen an jedem Ende für spektakuläre Sprünge und Tore.\n",
            "\n",
            "Frage: Im Kontext des Balls, aus welchem Material besteht er und wie schwer ist er?\n",
            "Antwort: Der Ball ist fünfzehneckig, aus einem flubberähnlichen Material, das unberechenbar springt, und wiegt 500 g.\n",
            "\n",
            "Frage: Im Kontext der Mannschaftsaufstellung, wie viele Spieler hat jede Mannschaft und welche Positionen gibt es?\n",
            "Antwort: Jede Mannschaft besteht aus 7 Spielern, darunter ein Torwart, der auf Stelzen steht, und zwei Flügelspieler mit kleinen Flügeln, um kurzzeitig zu schweben.\n",
            "\n",
            "Frage: Im Kontext der Ausrüstung der Spieler, welche Kleidung tragen die Spieler?\n",
            "Antwort: Spieler tragen bunte Kostüme mit integrierten LED-Lampen.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem Text über das Spassball Spiel:\n",
            "\n",
            "Frage: Im Kontext des Spielfelds, welche Besonderheiten gibt es?\n",
            "Antwort: Das Spielfeld ist oval mit zwei Trampolinen an jedem Ende für spektakuläre Sprünge und Tore.\n",
            "\n",
            "Frage: Im Kontext des Balls, aus welchem Material besteht er und wie schwer ist er?\n",
            "Antwort: Der Ball ist fünfzehneckig, aus einem flubberähnlichen Material, das unberechenbar springt, und wiegt 500 g.\n",
            "\n",
            "Frage: Im Kontext der Mannschaftsaufstellung, wie viele Spieler hat jede Mannschaft und welche Positionen gibt es?\n",
            "Antwort: Jede Mannschaft besteht aus 7 Spielern, darunter ein Torwart, der auf Stelzen steht, und zwei Flügelspieler mit kleinen Flügeln, um kurzzeitig zu schweben.\n",
            "\n",
            "Frage: Im Kontext der Ausrüstung der Spieler, welche Kleidung tragen die Spieler?\n",
            "Antwort: Spieler tragen bunte Kostüme mit integrierten LED-Lampen.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "Frage: Im Kontext des Spiels, was passiert bei einem Foul?\n",
            "Antwort: Bei einem Foul blinken die Schienbeinschützer.\n",
            "\n",
            "Frage: Im Kontext der Ausrüstung, was haben Schienbeinschützer integriert?\n",
            "Antwort: Schienbeinschützer haben integrierte Lautsprecher, die lustige Töne abspielen.\n",
            "\n",
            "Frage: Wie bewegt sich der Schiedsrichter während des Spiels?\n",
            "Antwort: Der Schiedsrichter reitet auf einem kleinen elektrischen Fahrzeug, um das schnelle Spiel zu verfolgen.\n",
            "\n",
            "Frage: Welches Werkzeug verwendet der Schiedsrichter, um Tore zu signalisieren?\n",
            "Antwort: Der Schiedsrichter verwendet eine Konfetti-Kanone, um Tore zu signalisieren.\n",
            "\n",
            "Frage: Wie ist die Struktur des Spiels in Bezug auf die Dauer der Drittel?\n",
            "Antwort: Das Spiel besteht aus drei Dritteln zu je 20 Minuten.\n",
            "\n",
            "Frage: Was passiert während der Pause zwischen den Dritteln?\n",
            "Antwort: Zwischen den Dritteln findet eine 10-minütige Pause statt, in der eine Mini-Tanzparty auf dem Spielfeld stattfindet.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "Frage: Im Kontext des Spiels, was passiert bei einem Foul?\n",
            "Antwort: Bei einem Foul blinken die Schienbeinschützer.\n",
            "\n",
            "Frage: Im Kontext der Ausrüstung, was haben Schienbeinschützer integriert?\n",
            "Antwort: Schienbeinschützer haben integrierte Lautsprecher, die lustige Töne abspielen.\n",
            "\n",
            "Frage: Wie bewegt sich der Schiedsrichter während des Spiels?\n",
            "Antwort: Der Schiedsrichter reitet auf einem kleinen elektrischen Fahrzeug, um das schnelle Spiel zu verfolgen.\n",
            "\n",
            "Frage: Welches Werkzeug verwendet der Schiedsrichter, um Tore zu signalisieren?\n",
            "Antwort: Der Schiedsrichter verwendet eine Konfetti-Kanone, um Tore zu signalisieren.\n",
            "\n",
            "Frage: Wie ist die Struktur des Spiels in Bezug auf die Dauer der Drittel?\n",
            "Antwort: Das Spiel besteht aus drei Dritteln zu je 20 Minuten.\n",
            "\n",
            "Frage: Was passiert während der Pause zwischen den Dritteln?\n",
            "Antwort: Zwischen den Dritteln findet eine 10-minütige Pause statt, in der eine Mini-Tanzparty auf dem Spielfeld stattfindet.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "Frage: Im Kontext des Spiels, wie beginnt es?\n",
            "Antwort: Das Spiel beginnt mit einem Witz -Duell in der Mitte des Spielfelds.\n",
            "\n",
            "Frage: Im Kontext des Witz-Duells, was gewinnt der Gewinner?\n",
            "Antwort: Der Gewinner darf den Ball zuerst treten.\n",
            "\n",
            "Frage: Im Kontext des Wiedereinstiegs nach jedem Tor, was wird getan?\n",
            "Antwort: Nach jedem Tor erfolgt der Wiedereinstieg durch einen Slapstick-Tanz am Mittelpunkt.\n",
            "\n",
            "Frage: Im Kontext des Balls aus dem Spiel, wann ist der Ball aus dem Spiel?\n",
            "Antwort: Der Ball ist aus dem Spiel, wenn er leuchtet und selbstständig zurück zum Mittelpunkt rollt oder wenn ein Lied gespielt wird.\n",
            "\n",
            "Frage: Im Kontext der Methode des Punktescores, wie werden Punkte erzielt?\n",
            "Antwort: Punkte werden erzielt, indem der Ball in ein sich bewegendes Tor geworfen wird, das zufällig die P\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "Frage: Im Kontext des Spiels, wie beginnt es?\n",
            "Antwort: Das Spiel beginnt mit einem Witz -Duell in der Mitte des Spielfelds.\n",
            "\n",
            "Frage: Im Kontext des Witz-Duells, was gewinnt der Gewinner?\n",
            "Antwort: Der Gewinner darf den Ball zuerst treten.\n",
            "\n",
            "Frage: Im Kontext des Wiedereinstiegs nach jedem Tor, was wird getan?\n",
            "Antwort: Nach jedem Tor erfolgt der Wiedereinstieg durch einen Slapstick-Tanz am Mittelpunkt.\n",
            "\n",
            "Frage: Im Kontext des Balls aus dem Spiel, wann ist der Ball aus dem Spiel?\n",
            "Antwort: Der Ball ist aus dem Spiel, wenn er leuchtet und selbstständig zurück zum Mittelpunkt rollt oder wenn ein Lied gespielt wird.\n",
            "\n",
            "Frage: Im Kontext der Methode des Punktescores, wie werden Punkte erzielt?\n",
            "Antwort: Punkte werden erzielt, indem der Ball in ein sich bewegendes Tor geworfen wird, das zufällig die P\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "1. **Frage:** Im Kontext des Spiels, was gibt es statt eines Abseits?\n",
            "   **Antwort:** Abseits gibt es nicht, dafür aber eine \"Narrenzone\", in der Spieler besondere Tricks ausführen müssen, um den Ball zu erhalten.\n",
            "\n",
            "2. **Frage:** Im Kontext von Fouls und unsportlichem Verhalten, was muss ein fehlbarer Spieler tun?\n",
            "   **Antwort:** Bei einem Foul muss der fehlbare Spieler ein kleines Gedicht über Fairplay vortragen.\n",
            "\n",
            "3. **Frage:** Im Kontext von Fouls und unsportlichem Verhalten, welche Strafe gibt es für unsportliches Verhalten?\n",
            "   **Antwort:** Bei unsportlichem Verhalten folgt eine Runde auf dem \"Schande-Stuhl\", der in der Mitte des Spielfelds steht.\n",
            "\n",
            "4. **Frage:** Im Kontext von Freistößen, wie werden diese entschieden?\n",
            "   **Antwort:** Freistöße werden durch ein kurzes Tanzduell entschieden, der Sieger darf den Ball spielen.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "1. **Frage:** Im Kontext des Spiels, was gibt es statt eines Abseits?\n",
            "   **Antwort:** Abseits gibt es nicht, dafür aber eine \"Narrenzone\", in der Spieler besondere Tricks ausführen müssen, um den Ball zu erhalten.\n",
            "\n",
            "2. **Frage:** Im Kontext von Fouls und unsportlichem Verhalten, was muss ein fehlbarer Spieler tun?\n",
            "   **Antwort:** Bei einem Foul muss der fehlbare Spieler ein kleines Gedicht über Fairplay vortragen.\n",
            "\n",
            "3. **Frage:** Im Kontext von Fouls und unsportlichem Verhalten, welche Strafe gibt es für unsportliches Verhalten?\n",
            "   **Antwort:** Bei unsportlichem Verhalten folgt eine Runde auf dem \"Schande-Stuhl\", der in der Mitte des Spielfelds steht.\n",
            "\n",
            "4. **Frage:** Im Kontext von Freistößen, wie werden diese entschieden?\n",
            "   **Antwort:** Freistöße werden durch ein kurzes Tanzduell entschieden, der Sieger darf den Ball spielen.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "### Ein Strafstoß wird von der Mittellinie aus durchgeführt, wobei der schießende Spieler dabei einen lustigen Hut tragen muss.\n",
            "Frage: Im Kontext eines Strafstoßes, wo wird dieser ausgeführt?\n",
            "Antwort: Ein Strafstoß wird von der Mittellinie aus durchgeführt.\n",
            "\n",
            "Frage: Im Kontext eines Strafstoßes, welche besondere Anforderung gibt es für den schießenden Spieler?\n",
            "Antwort: Der schießende Spieler muss dabei einen lustigen Hut tragen.\n",
            "\n",
            "### Einwurf: Einwürfe werden ersetzt durch Eintritte - der Ball wird mit einem kunstvollen Sprung zurück ins Spiel gebracht.\n",
            "Frage: Im Kontext eines Einwurfs, was ersetzt diesen?\n",
            "Antwort: Einwürfe werden ersetzt durch Eintritte.\n",
            "\n",
            "Frage: Im Kontext eines Eintritts, wie wird der Ball ins Spiel gebracht?\n",
            "Antwort: Der Ball wird mit einem kunstvollen Sprung zurück ins Spiel gebracht.\n",
            "\n",
            "### Eckstoß und Abstoß: Eckstöße werden als Rätselrunden ausgeführt, bei denen das angreifende Team ein Rätsel lösen muss. Abstöße erfolgen durch einen kraftvollen Gähn -Laut des Torwarts.\n",
            "Frage: Im Kontext eines Eckstoßes, wie wird dieser ausgeführt?\n",
            "Antwort: Eckstöße werden als Rätselrunden ausgeführt, bei denen das angreifende Team ein Rätsel lösen muss.\n",
            "\n",
            "Frage: Im Kontext eines Abstoßes, wie erfolgt dieser?\n",
            "Antwort: Abstöße erfolgen durch einen kraftvollen Gähn -Laut des Torwarts.\n",
            "\n",
            "### Schiedsrichterball: Statt eines Schiedsrichterballs gibt es e\n",
            "Frage: Im Kontext eines Schiedsrichterballs, was gibt es stattdessen?\n",
            "Antwort: Statt eines Schiedsrichterballs gibt es ein E.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "### Ein Strafstoß wird von der Mittellinie aus durchgeführt, wobei der schießende Spieler dabei einen lustigen Hut tragen muss.\n",
            "Frage: Im Kontext eines Strafstoßes, wo wird dieser ausgeführt?\n",
            "Antwort: Ein Strafstoß wird von der Mittellinie aus durchgeführt.\n",
            "\n",
            "Frage: Im Kontext eines Strafstoßes, welche besondere Anforderung gibt es für den schießenden Spieler?\n",
            "Antwort: Der schießende Spieler muss dabei einen lustigen Hut tragen.\n",
            "\n",
            "### Einwurf: Einwürfe werden ersetzt durch Eintritte - der Ball wird mit einem kunstvollen Sprung zurück ins Spiel gebracht.\n",
            "Frage: Im Kontext eines Einwurfs, was ersetzt diesen?\n",
            "Antwort: Einwürfe werden ersetzt durch Eintritte.\n",
            "\n",
            "Frage: Im Kontext eines Eintritts, wie wird der Ball ins Spiel gebracht?\n",
            "Antwort: Der Ball wird mit einem kunstvollen Sprung zurück ins Spiel gebracht.\n",
            "\n",
            "### Eckstoß und Abstoß: Eckstöße werden als Rätselrunden ausgeführt, bei denen das angreifende Team ein Rätsel lösen muss. Abstöße erfolgen durch einen kraftvollen Gähn -Laut des Torwarts.\n",
            "Frage: Im Kontext eines Eckstoßes, wie wird dieser ausgeführt?\n",
            "Antwort: Eckstöße werden als Rätselrunden ausgeführt, bei denen das angreifende Team ein Rätsel lösen muss.\n",
            "\n",
            "Frage: Im Kontext eines Abstoßes, wie erfolgt dieser?\n",
            "Antwort: Abstöße erfolgen durch einen kraftvollen Gähn -Laut des Torwarts.\n",
            "\n",
            "### Schiedsrichterball: Statt eines Schiedsrichterballs gibt es e\n",
            "Frage: Im Kontext eines Schiedsrichterballs, was gibt es stattdessen?\n",
            "Antwort: Statt eines Schiedsrichterballs gibt es ein E.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "### Kurze Pantomime-Einlage:\n",
            "Frage: Im Kontext des Spiels, was ist die kurze Pantomime-Einlage und wie wird sie beendet?\n",
            "Antwort: Eine kurze Pantomime-Einlage ist eine Einlage, nach deren Ende das Spiel mit einem lauten Lachen aller Spieler fortgesetzt wird.\n",
            "\n",
            "### Magischer Moment:\n",
            "Frage: Im Kontext des Spiels, was ist der \"Magische Moment\" und wie oft kann er pro Spiel eingesetzt werden?\n",
            "Antwort: Der \"Magische Moment\" ist eine spezielle Aktion, bei der der Ball für fünf Sekunden unsichtbar wird. Jedes Team darf diesen Moment einmal pro Spiel einsetzen.\n",
            "\n",
            "### Herausgegeben von der SIFA:\n",
            "Frage: Im Kontext des Textes, wer hat den Text herausgegeben und wann?\n",
            "Antwort: Der Text wurde herausgegeben von der SIFA, der Spaßball International Federation Association, am 07.03.2020.\n",
            "--------------------------------------------------------------------------------\n",
            "Aktuelle Q&A:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem gegebenen Text:\n",
            "\n",
            "### Kurze Pantomime-Einlage:\n",
            "Frage: Im Kontext des Spiels, was ist die kurze Pantomime-Einlage und wie wird sie beendet?\n",
            "Antwort: Eine kurze Pantomime-Einlage ist eine Einlage, nach deren Ende das Spiel mit einem lauten Lachen aller Spieler fortgesetzt wird.\n",
            "\n",
            "### Magischer Moment:\n",
            "Frage: Im Kontext des Spiels, was ist der \"Magische Moment\" und wie oft kann er pro Spiel eingesetzt werden?\n",
            "Antwort: Der \"Magische Moment\" ist eine spezielle Aktion, bei der der Ball für fünf Sekunden unsichtbar wird. Jedes Team darf diesen Moment einmal pro Spiel einsetzen.\n",
            "\n",
            "### Herausgegeben von der SIFA:\n",
            "Frage: Im Kontext des Textes, wer hat den Text herausgegeben und wann?\n",
            "Antwort: Der Text wurde herausgegeben von der SIFA, der Spaßball International Federation Association, am 07.03.2020.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# WICHTIG!!!! Zwischenschritt\n",
        "\n",
        "1.   Q&A-Datensatz herunterladen (data_qa.json)\n",
        "2.   Datensatz überarbeiten (fehlerhafte Elemente löschen/anpassen, bereinigen)\n",
        "3.   Notebook zurücksetzen\n",
        "4.   Datensatz hochladen\n",
        "5.   Notebook neu starten. Wieder mit T4-GPU\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "x00alEtYXN8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Finetuning\n",
        "\n",
        "Das folgende Skript geht zurück auf [Unsloth](https://github.com/unslothai/unsloth). Einem aktuellen Framework um auch mit wenigen Ressourcen schnell qualitativ hochwertige Ergebnisse zu erzielen.\n",
        "\n",
        "Hier der Link zum Original-Notebook von [Mistral7b-Unsloth](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)"
      ],
      "metadata": {
        "id": "S1Uow-0xWFJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Installation nötiger Bibs und Bezug des Modells\n",
        "\n",
        "\n",
        "\n",
        "*  Zunächst müssen relevante Python-Bibs bezogen werden.\n",
        "*  Das Modell Mistral-7b wird heruntergeladen und in 4bit bereitgestellt.\n",
        "*  Das Lora-Adapter-Template wird heruntergeladen um für das Finetuning trainiert zu werden.\n",
        "*  Der Datensatz wird geladen und in das passende Format übertragen"
      ],
      "metadata": {
        "id": "uJdvqOdcjUA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "20EHnQv_W_37"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "pobd-hTNX7iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e82f5c-d0b0-480b-e973-49749aa0331e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.11.10 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Laden des lokalen Datensatzes\n",
        "with open('data_qa.json', 'r', encoding='utf-8') as file:\n",
        "    data_qa = json.load(file)\n",
        "\n",
        "EOS_TOKEN = \"[EOS]\"  # Beispiel für ein EOS-Token, anpassen nach Bedarf\n",
        "\n",
        "def formatting_prompts_func(data_qa):\n",
        "    texts = []\n",
        "    for qa_pair in data_qa:\n",
        "        # Direkte Verwendung des QA-Paares als \"instruction\" und \"output\", \"input\" bleibt leer\n",
        "        instruction = \"Bitte umformulieren der folgenden Frage und Antwort in ein klares Format.\"\n",
        "        input = \"\"  # Kein spezifischer Eingabetext, da der QA-Text bereits die Information enthält\n",
        "        output = qa_pair  # Direkte Verwendung des QA-Paares aus dem Datensatz\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\" + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# Verarbeitung und Formatierung des Datensatzes\n",
        "formatted_data = formatting_prompts_func(data_qa)\n",
        "\n",
        "# Beispielweise Ausgabe eines formatierten Elements zur Überprüfung\n",
        "print(formatted_data[0])\n"
      ],
      "metadata": {
        "id": "aQ63ZCV5X8yT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9253a69d-41d9-4d27-eb13-9bd608c19831"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Bitte umformulieren der folgenden Frage und Antwort in ein klares Format.\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            " Hier sind die Frage-Antwort-Paare basierend auf dem Text über das Spassball Spiel:\n",
            "\n",
            "Frage: Im Kontext des Spielfelds, welche Besonderheiten gibt es?\n",
            "Antwort: Das Spielfeld ist oval mit zwei Trampolinen an jedem Ende für spektakuläre Sprünge und Tore.\n",
            "\n",
            "Frage: Im Kontext des Balls, aus welchem Material besteht er und wie schwer ist er?\n",
            "Antwort: Der Ball ist fünfzehneckig, aus einem flubberähnlichen Material, das unberechenbar springt, und wiegt 500 g.\n",
            "\n",
            "Frage: Im Kontext der Mannschaftsaufstellung, wie viele Spieler hat jede Mannschaft und welche Positionen gibt es?\n",
            "Antwort: Jede Mannschaft besteht aus 7 Spielern, darunter ein Torwart, der auf Stelzen steht, und zwei Flügelspieler mit kleinen Flügeln, um kurzzeitig zu schweben.\n",
            "\n",
            "Frage: Im Kontext der Ausrüstung der Spieler, welche Kleidung tragen die Spieler?\n",
            "Antwort: Spieler tragen bunte Kostüme mit integrierten LED-Lampen.[EOS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Angenommen, formatted_data ist eine Liste von formatierten Texten\n",
        "# Beispiel für formatted_data:\n",
        "# formatted_data = [\"### Instruction:\\nFrage ... Antwort ... [EOS]\", \"...\"]\n",
        "\n",
        "# Konvertieren von formatted_data in ein Dataset-Objekt\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n"
      ],
      "metadata": {
        "id": "nHZ1jBE5Zuu8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Training\n",
        "\n",
        "(das Training kann 10-30 Minuten dauern, je nach Größe des Datensatzes auch länger)\n",
        "\n",
        "*  Hyperparameter für das Training (können angepasst werden um das Training zu optimieren)\n",
        "*  Darstellen der Ressourcen vor und nach dem Training\n",
        "*  Trainingsverlauf (hier den angegebenen Wert beachten. Dies ist der Verlustwert und er sollte gegen 0 gehen. Sollte dieser steigen, so wäre es nachteilig für das Training)"
      ],
      "metadata": {
        "id": "WbQIj0zpkSCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Konvertieren von formatted_data in ein Dataset-Objekt (vorausgesetzt, formatted_data ist eine Liste von Strings)\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "# Anschließende Anpassungen und Initialisierung des Trainers\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,  # Verwenden Sie das konvertierte Dataset-Objekt hier\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=128,  # Stellen Sie sicher, dass diese Zahl geeignet ist für Ihre Daten\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "feWUHsO2Zet1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f2689120816b414c94e0c45b6d160140",
            "226fb0ff1c0448f5bca72b26f59f66ed",
            "6a3ec42391844aabaa502a87ebfc6441",
            "9b474272a06447479953a5a524e5fd2c",
            "4de4193142ba477ab0326cb0cc6af469",
            "d6536f96c4c5481c8b124a407f81156d",
            "e720a359ab504c83857d4dd3a0f6839d",
            "db89ff6a63ec4446b35323a4e382a32b",
            "6171f4d62a264c9dbe7c34ef19094b1e",
            "d7b8c83be0994ffc9d1e0cb7adcd27e7",
            "f3a343325f93456eb282612197c98d97"
          ]
        },
        "outputId": "01a5e15d-71d3-449e-d73e-ca1443140ea5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2689120816b414c94e0c45b6d160140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "Ck1Odi8daGwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f83b7f-a456-4d2a-a867-d5b45faba5b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "4.006 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "oq9AjAfeaJKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "73e62bcb-e1bc-4217-9775-1bb8edf43f95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 6 | Num Epochs = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2/60 : < :, Epoch 1.67/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3d62c575fcfd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/tokenizer_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2998\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_flos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3494\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, logs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_prediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    519\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_log\u001b[0;34m(self, args, state, control, model, logs, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_value_scalars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mnon_scalar_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnon_scalar_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train/global_step\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats (optional)\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "6qm_TlFZaM8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Speichern des Modells und des Adapters\n",
        "\n",
        "(dies kann bis 20 Minuten dauern)\n",
        "\n",
        "*  Zunächst wird der trainierte Adapter gespeichert\n",
        "*  Nun wird das Modell im 4b-quantisierten Format *.gguf gespeichert"
      ],
      "metadata": {
        "id": "1qCC8cl7kwSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# Save to q4_k_m GGUF\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "metadata": {
        "id": "rQ4H1v41bUdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# WICHTIG!!!! Zwischenschritt\n",
        "\n",
        "1.   model-unsloth.Q4_K_M.gguf herunterladen\n",
        "2.   Notebook mit T4-GPU neu starten\n",
        "3.   model-unsloth.Q4_K_M.gguf hochladen\n",
        "4.   Ab hier weitermachen!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "57Fr7Hi2gNBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Inferenz (Test)"
      ],
      "metadata": {
        "id": "5X0F26PTa8f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Testvorbereitungen\n",
        "\n",
        "\n",
        "\n",
        "*  Instalation aller relevanten Python-Bibs für Ollama\n",
        "*  Erstellen eines Ollama-Modells basierend auf dem trainierten *.gguf-Modells"
      ],
      "metadata": {
        "id": "j-XFl3AllS2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm model-unsloth.F16.gguf\n",
        "!rm -r model\n",
        "!rm -r lora_model\n",
        "!rm -r llama.cpp\n",
        "!rm -r outputs"
      ],
      "metadata": {
        "id": "NnVmDud_ul6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! sudo apt-get install -y pciutils\n",
        "!pip install pypdf2\n",
        "! curl https://ollama.ai/install.sh | sh\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ],
      "metadata": {
        "id": "u0mqg5aRfYwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Dateiname definieren\n",
        "filename = 'Modelfile'\n",
        "\n",
        "# Inhalt der Datei\n",
        "content = 'FROM ./model-unsloth.Q4_K_M.gguf'\n",
        "\n",
        "# Datei erstellen und schreiben\n",
        "with open(filename, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(f\"Datei '{filename}' wurde erfolgreich erstellt.\")\n",
        "\n",
        "!ollama create ft_mistral -f Modelfile\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Befehl definieren\n",
        "command = 'ollama create ft_mistral -f Modelfile'\n",
        "\n",
        "# Befehl ausführen\n",
        "process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Ausgabe und Fehlerausgabe (falls vorhanden) anzeigen\n",
        "print(process.stdout.decode())\n",
        "if process.stderr:\n",
        "    print(\"Fehler:\", process.stderr.decode())\n"
      ],
      "metadata": {
        "id": "6Ql-ynn0bJap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Testdurchführung mit eigenen Prompts\n",
        "\n",
        "\n",
        "\n",
        "*  Eigenen Prompt eingeben um das Modell zu testen"
      ],
      "metadata": {
        "id": "UOP6-WkblpVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benutzeraufforderung für den gesamten Prompt\n",
        "prompt = input(\"Bitte geben Sie Ihren Prompt ein: \")\n",
        "\n",
        "# Ausgabe des Prompts zur Überprüfung\n",
        "print(\"Ihr eingegebener Prompt lautet:\")\n",
        "print(prompt)\n",
        "\n",
        "url = 'http://localhost:11434/api/chat'\n",
        "payload = {\n",
        "    \"model\": \"ft_mistral\",\n",
        "    \"temperature\": 0.6,\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher KI-Assistent!\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "message_str = response.content.decode('utf-8')\n",
        "message_dict = json.loads(message_str)\n",
        "print(message_dict['message']['content'])"
      ],
      "metadata": {
        "id": "_cMZMu3jfghn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}