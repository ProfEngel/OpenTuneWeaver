{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx3qhYa3v2oA3HPN6G+eck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfEngel/FineTuneLLM/blob/main/01_Parseandprepare_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisierung des RunPod-Pods samt Image\n",
        "\n",
        "Api-Key von Runpod bereitstellen"
      ],
      "metadata": {
        "id": "xMvsvMRfKWMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QccZx3ILKQrK",
        "outputId": "c55ee9a9-e6bf-4aab-ea6f-e983b7650bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rrunpodctl             0%[                    ]       0  --.-KB/s               \rrunpodctl            85%[================>   ]  10.04M  50.2MB/s               \rrunpodctl           100%[===================>]  11.81M  54.5MB/s    in 0.2s    \n",
            "Gib den Runpod-ApiKey einAY7OXSUOGY41VK8IQIRYGYHI3Z59E6NETY8852B0\n",
            "saved apiKey into config file: /root/.runpod.yaml\n",
            "pod \"gpcq1dt23kg6qg\" created for $1.580 / hr\n"
          ]
        }
      ],
      "source": [
        "#@title 1. RunPod Initialisierung\n",
        "!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.10.0/runpodctl-linux-amd -O runpodctl\n",
        "!chmod +x runpodctl\n",
        "!cp runpodctl /usr/bin/runpodctl\n",
        "\n",
        "rp_key = input(\"Gib den Runpod-ApiKey ein\")\n",
        "!runpodctl config --apiKey {rp_key}\n",
        "#!runpodctl create pod --gpuType \"NVIDIA RTX A6000\" --secureCloud --gpuCount 2 --templateId bofievo43g --imageName ghcr.io/huggingface/text-generation-inference:latest\n",
        "\n",
        "!runpodctl create pod \\\n",
        "--gpuType \"NVIDIA RTX A6000\" \\\n",
        "--secureCloud \\\n",
        "--gpuCount 2 \\\n",
        "--templateId bofievo43g \\\n",
        "--imageName ghcr.io/huggingface/text-generation-inference:latest \\\n",
        "--containerDiskSize 100 \\\n",
        "--volumeSize 100 \\\n",
        "--volumePath \"/workspace\" \\\n",
        "--ports \"8080/http\" \\\n",
        "--args \"--model-id TheBloke/Llama-2-70B-chat-AWQ --trust-remote-code --port 8080 --max-input-length 2048 --max-total-tokens 4096 --max-batch-prefill-tokens 4096 --quantize awq --speculate 3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Initialisierung der Bibs\n",
        "%%capture\n",
        "# Überprüfung, ob eine requirements.txt-Datei bereits existiert und deren Löschung\n",
        "!if [ -f requirements.txt ]; then rm requirements.txt; fi\n",
        "# Herunterladen der neuen requirements.txt-Datei\n",
        "!wget https://raw.githubusercontent.com/ProfEngel/FineTuneLLM/main/requirements.txt\n",
        "# Installation der Abhängigkeiten aus der heruntergeladenen requirements.txt-Datei\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Rg1IIpCoKhvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Abfrage der relevanten Daten\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Erstellen Sie den Ordner 'data', falls er nicht existiert\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Fordern Sie den Benutzer zur Hochladung einer Datei auf\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Speichern Sie die hochgeladene Datei unter 'data/train.pdf'\n",
        "for filename in uploaded.keys():\n",
        "    with open(f'data/train.pdf', 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "    print(f\"'{filename}' wurde als 'data/train.pdf' gespeichert.\")\n",
        "    break  # Beendet die Schleife nach der Speicherung der ersten hochgeladenen Datei\n",
        "\n",
        "# Benutzereingaben anfordern\n",
        "kontext_laenge = input(\"Wie lange soll die Kontext-Länge sein? (Empfohlen: 1000) \")\n",
        "kontext = input(\"Worum geht es in diesem Datensatz? (Bsp. 'Entscheidungsökonomik. Definitionen, Ablauf und Verständnisfragen. Auch mit Beispielen') \")\n",
        "\n",
        "# Konvertieren Sie die Eingabe für die Kontextlänge in einen Integer, falls nötig\n",
        "try:\n",
        "    kontext_laenge = int(kontext_laenge)\n",
        "except ValueError:\n",
        "    print(\"Die eingegebene Kontextlänge ist keine gültige Zahl. Es wird der empfohlene Wert 1000 verwendet.\")\n",
        "    kontext_laenge = 1000\n",
        "\n",
        "# Variablen im Quellcode zuweisen\n",
        "model_context_length = kontext_laenge\n",
        "context = f'Context: {kontext}'\n",
        "\n",
        "# Beispiel für den Benutzer anzeigen\n",
        "print(\"Bitte geben Sie ein Trainings- und ein Testbeispiel im folgenden Format ein:\")\n",
        "print(\"Trainingsbeispiel:\")\n",
        "print(\"Wie erkennt man ein Nash-Gleichgewicht?\\nEin Nash-Gleichgewicht ist erreicht, wenn die Auszahlungen für die Strategien der Spieler in einer Weise kombiniert sind, dass keiner der Spieler durch einseitiges Abweichen von seiner Strategie eine bessere Auszahlung erzielen kann.\\n\")\n",
        "print(\"Testbeispiel:\")\n",
        "print(\"Was bedeutet das Nash-Gleichgewicht für die Entscheidungsfindung der Spieler?\\nEs bedeutet, dass in Abhängigkeit von der Wahl des Gegenspielers ein Spieler die optimale Entscheidung trifft, indem er zwischen allen seinen möglichen Strategien wählt, ohne seine Entscheidung nachträglich ändern zu wollen, da jede Änderung zu einer schlechteren Auszahlung führen würde.\\n\")\n",
        "\n",
        "# Funktion zur Formatierung der Eingabe\n",
        "def format_sample(prompt_type):\n",
        "    question = input(f\"Bitte geben Sie die Frage für Ihr {prompt_type}-Beispiel ein: \")\n",
        "    answer = input(f\"Bitte geben Sie die Antwort für Ihr {prompt_type}-Beispiel ein: \")\n",
        "    formatted_sample = f\"{question}\\n{answer}\\n\"\n",
        "    return formatted_sample\n",
        "\n",
        "# Benutzer zur Eingabe für das Trainingsbeispiel auffordern\n",
        "train_sample = format_sample(\"Trainings\")\n",
        "print(\"\\nIhr formatiertes Trainingsbeispiel:\")\n",
        "print(train_sample)\n",
        "\n",
        "# Benutzer zur Eingabe für das Testbeispiel auffordern\n",
        "test_sample = format_sample(\"Test\")\n",
        "print(\"\\nIhr formatiertes Testbeispiel:\")\n",
        "print(test_sample)"
      ],
      "metadata": {
        "id": "lTYAoanIKwfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = f\"https://{pod_id}-8080.proxy.runpod.net/generate\"\n",
        "print(url)  # Geben Sie die URL vor der Anfrage aus, um sie zu überprüfen.\n",
        "\n",
        "\n",
        "payload = {\n",
        "    \"inputs\": \"[INST] Geben Sie hier Ihren Test-Prompt ein. [/INST]\\n\\n\",\n",
        "    \"parameters\": {\n",
        "        \"max_new_tokens\": 50,\n",
        "        \"do_sample\": False,\n",
        "        \"stop\": [\"</s>\", \"[INST]\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Erfolgreiche Verbindung und Antwort:\", response.text)\n",
        "else:\n",
        "    print(f\"Fehler: HTTP-Status {response.status_code}, Antwort: {response.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENuvK9MYvf1G",
        "outputId": "ed9196a1-994d-4801-9b6c-5e78af1afe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://gpcq1dt23kg6qg-8080.proxy.runpod.net/generate\n",
            "Erfolgreiche Verbindung und Antwort: {\"generated_text\":\"Sure! Here's a prompt for a text-based conversation:\\n\\nYou: Hi there! How can I help you today?\\n\\nUser: Hi! I'm looking for a new restaurant to try in the city. Do\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Datenübergabe von IDs\n",
        "import subprocess\n",
        "\n",
        "!runpodctl get pod\n",
        "output = subprocess.run([\"runpodctl\", \"get\", \"pod\"], stdout=subprocess.PIPE).stdout.decode(\"utf-8\")\n",
        "pod_id = output.split()[6]\n",
        "print(pod_id)\n",
        "# Pfad zur .env-Datei\n",
        "env_file_path = \".env\"\n",
        "\n",
        "# Überprüfen, ob die .env-Datei existiert, und sie löschen\n",
        "if os.path.exists(env_file_path):\n",
        "    os.remove(env_file_path)\n",
        "    print(f\"Die Datei {env_file_path} wurde gelöscht.\")\n",
        "\n",
        "# Fordern Sie den Nutzer zur Eingabe auf\n",
        "runpod_pod_id = pod_id\n",
        "pod_id=runpod_pod_id\n",
        "\n",
        "# Erstellen Sie die .env-Datei und schreiben Sie die Variable hinein\n",
        "with open(\".env\", \"w\") as file:\n",
        "    file.write(f\"RUNPOD_POD_ID={runpod_pod_id}\")\n",
        "\n",
        "print(\".env-Datei erfolgreich erstellt.\")"
      ],
      "metadata": {
        "id": "YwBRAhkgLDNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "62502513-1183-4faf-dc0c-5433a1a75687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID            \tNAME                                         \tGPU        \tIMAGE NAME                                          \tSTATUS  \n",
            "gpcq1dt23kg6qg\tghcr.io/huggingface/text-generation-inference\t2 RTX A6000\tghcr.io/huggingface/text-generation-inference:latest\tRUNNING\t\n",
            "gpcq1dt23kg6qg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3cf43d4ccc5d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Überprüfen, ob die .env-Datei existiert, und sie löschen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Die Datei {env_file_path} wurde gelöscht.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. PDF zu Q&A\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    with open(txt_path, 'w') as f:\n",
        "        f.write(text)\n",
        "\n",
        "def main():\n",
        "    summary = []\n",
        "    paths_to_check = ['data/train.pdf', 'data/test.pdf']\n",
        "\n",
        "    for pdf_path in paths_to_check:\n",
        "        # Extract the base name to create txt file name\n",
        "        base_name = os.path.basename(pdf_path).split('.')[0]\n",
        "        txt_path = f'data/raw_{base_name}.txt'\n",
        "\n",
        "        # Check if PDF exists\n",
        "        if os.path.exists(pdf_path):\n",
        "            pdf_to_text(pdf_path, txt_path)\n",
        "            summary.append(f\"Converted {pdf_path} to {txt_path}.\")\n",
        "        else:\n",
        "            summary.append(f\"{pdf_path} was not found.\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Summary:\")\n",
        "    for item in summary:\n",
        "        print(f\"- {item}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import tiktoken\n",
        "import requests\n",
        "import json\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "pod_id = os.getenv(\"RUNPOD_POD_ID\")\n",
        "print(f\"Verwendete POD_ID: {pod_id}\")\n",
        "\n",
        "\n",
        "# context\n",
        "#model_context_length = 1000 #don't make this too small or the model will be dealing with a lot of chopped input.\n",
        "#context = 'Context: Stanford Name, Image and Likeness Disclosure Policies.'\n",
        "tokens_per_question = 30 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
        "chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question) # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
        "# chunk_size = 200 # for testing, set a smaller chunk size.\n",
        "\n",
        "#train_sample = \"According to Stanford NIL policies, may a student-athlete miss class to engage in NIL related activity??\\nStudent-athletes need to work with their professors to seek approval for any missed class time to engage in NIL activities. Stanford Athletics is not able to provide approval for student-athletes missing class.\"\n",
        "#test_sample = \"Can entering a NIL deal affect a Stanford student's eligibility for scholarships?\\nNo, not as long as students follow California law and Stanford policy.\"\n",
        "\n",
        "questions_per_chunk_train = int(chunk_size / tokens_per_question)\n",
        "questions_per_chunk_test = max(int(questions_per_chunk_train / 10),1)\n",
        "\n",
        "print(f'Setting {questions_per_chunk_train} questions per {int(chunk_size)}-token chunk for QA train dataset generation.')\n",
        "\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    token_count = sum(1 for _ in encoding.encode(text))\n",
        "    return token_count\n",
        "\n",
        "def read_and_chunk_txt(file_path):\n",
        "    chunks = []\n",
        "    chunk = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip()\n",
        "            if count_tokens(chunk + text) > chunk_size:\n",
        "                chunks.append(chunk.strip())\n",
        "                chunk = text\n",
        "            else:\n",
        "                chunk += \" \" + text\n",
        "    if chunk:\n",
        "        chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "def query_runpod(pod_id, prompt, max_tokens):\n",
        "    url = f\"https://{pod_id}-8080.proxy.runpod.net/generate\"\n",
        "    prompt = f'[INST] {prompt} [/INST]\\n\\n'\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"do_sample\": False,\n",
        "            \"stop\": [\n",
        "                \"</s>\",\n",
        "                \"[INST]\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
        "    # print(url)\n",
        "    if response.status_code == 200:\n",
        "        return json.loads(response.text)[\"generated_text\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "load_dotenv()\n",
        "api_choice = input(\"Choose the API to use (openai/runpod): \").strip().lower()\n",
        "\n",
        "if api_choice == \"openai\":\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not openai.api_key:\n",
        "        print(\"OpenAI API key is missing. Exiting.\")\n",
        "        exit(1)\n",
        "elif api_choice == \"runpod\":\n",
        "    pod_id = os.getenv(\"RUNPOD_POD_ID\")\n",
        "    if not pod_id:\n",
        "        print(\"RunPod Pod ID is missing. Exiting.\")\n",
        "        exit(1)\n",
        "else:\n",
        "    print(\"Invalid API choice. Exiting.\")\n",
        "    exit(1)\n",
        "\n",
        "chunks = read_and_chunk_txt(\"data/raw_train.txt\")\n",
        "\n",
        "total_tokens = sum(count_tokens(chunk) for chunk in chunks)\n",
        "print(f\"Total tokens in all chunks: {total_tokens}\")\n",
        "\n",
        "estimated_input_tokens = total_tokens * 1.1\n",
        "estimated_output_tokens = total_tokens * 50/tokens_per_question\n",
        "total_estimated_tokens = estimated_input_tokens + estimated_output_tokens\n",
        "\n",
        "estimated_cost_gpt4 = (estimated_input_tokens / 1000 * 0.03) + (estimated_output_tokens  / 1000 * 0.06)\n",
        "estimated_cost_gpt35turbo = (estimated_input_tokens / 1000 * 0.003) + (estimated_output_tokens  / 1000 * 0.004)\n",
        "print(f\"Estimated cost with gpt-4: ${estimated_cost_gpt4:.2f}\")\n",
        "print(f\"Estimated cost with gpt-3.5-turbo-16k: ${estimated_cost_gpt35turbo:.2f}\")\n",
        "\n",
        "while True:\n",
        "    process_option = input(\"Do you want to process one chunk (type 'one') or all chunks (type 'all')? \").strip().lower()\n",
        "    if process_option in ['one', 'all']:\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid option. Please enter 'one' or 'all'.\")\n",
        "\n",
        "snippets = [\n",
        "    f\"Geben Sie {questions_per_chunk_train} Frage- und Antwortpaare auf der Grundlage des obigen Textes immer in Deutsch an. Die Fragen müssen mit \\\"Im Kontext von ...\\\" beginnen. Die Antworten sollten wortwörtlich aus dem obigen Text übernommen werden. Berücksichtigen Sie bei der Formulierung jeder Frage, dass der Leser die anderen Fragen nicht sieht und auch keinen Zugang zu ihnen hat, um den Kontext zu erkennen. Variieren Sie den Stil und das Format der Fragen. Beantworten Sie jede Frage und jede Antwort im Klartext in einer neuen Zeile. Geben Sie keine Fragenummern an. Hier ist ein Beispiel für zwei Frage-Antwort-Paare:\\n\\n{train_sample}\",\n",
        "        f\"Geben Sie {questions_per_chunk_test} Frage- und Antwortpaar(e) auf der Grundlage des obigen Textes an. Die Fragen müssen mit \\\"Im Kontext von ...\\\" beginnen. Die Antworten sollten NICHT wortwörtlich aus dem obigen Text übernommen werden, aber sie sollten die Bedeutung beibehalten. Berücksichtigen Sie bei der Formulierung jeder Frage, dass der Leser die anderen Fragen nicht sieht und auch keinen Zugang zu ihnen hat, um den Kontext zu verstehen. Variieren Sie den Stil und das Format der Fragen. Beantworten Sie jede Frage und jede Antwort im Klartext in einer neuen Zeile. Geben Sie keine Fragenummern an. Hier ist ein Beispiel für zwei Frage-Antwort-Paare:\\n\\n{test_sample}\"\n",
        "]\n",
        "\n",
        "for idx, snippet in enumerate(snippets):\n",
        "    print(snippet)\n",
        "    output_filename = \"data/train.txt\" if idx == 0 else \"data/test.txt\"\n",
        "\n",
        "    with open(output_filename, \"w\", encoding='utf-8') as output_file:\n",
        "        if api_choice == \"openai\":\n",
        "                for chunk_idx, chunk in enumerate(chunks):\n",
        "                    prompt = f\"{context}\\n\\n{chunk}\\n\\n{snippet}\"\n",
        "                    if process_option == 'one':\n",
        "                        print(f\"\\n\\n{prompt}\")\n",
        "\n",
        "                    if process_option == 'one' and chunk_idx > 0:\n",
        "                        break\n",
        "\n",
        "                    completion = openai.ChatCompletion.create(\n",
        "                        # model=\"gpt-4\",\n",
        "                        model=\"gpt-3.5-turbo-16k\",\n",
        "                        temperature=0,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent.\"},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    response = completion.choices[0].message['content']\n",
        "                    output_file.write(response + \"\\n\\n\")\n",
        "                    output_file.flush()\n",
        "\n",
        "                    time.sleep(0.2)\n",
        "        elif api_choice == \"runpod\":\n",
        "            max_tokens = int(model_context_length * 0.9)\n",
        "\n",
        "        if process_option == 'all':\n",
        "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "                future_to_chunk = {executor.submit(query_runpod, pod_id, f\"{context}\\n\\n{chunk}\\n\\n{snippet}\", max_tokens): chunk for chunk in chunks}\n",
        "                for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "                    chunk = future_to_chunk[future]\n",
        "                    try:\n",
        "                        response = future.result()\n",
        "                    except Exception as exc:\n",
        "                        print(f\"Generated an exception: {exc}\")\n",
        "                    else:\n",
        "                        # Überprüfen, ob 'response' einen gültigen Wert enthält, bevor Sie versuchen, ihn zu schreiben\n",
        "                        if response is not None:\n",
        "                            output_file.write(response + \"\\n\\n\")\n",
        "                            output_file.flush()\n",
        "                        else:\n",
        "                            print(f\"No response for chunk: {chunk}\")\n",
        "        else:\n",
        "            for chunk in chunks:\n",
        "                response = query_runpod(pod_id, f\"{context}\\n\\n{chunk}\\n\\n{snippet}\", max_tokens)\n",
        "                # Auch hier die Überprüfung\n",
        "                if response is not None:\n",
        "                    output_file.write(response + \"\\n\\n\")\n",
        "                    output_file.flush()\n",
        "                else:\n",
        "                    print(f\"No response for chunk: {chunk}\")\n",
        "\n",
        "                if process_option == 'one':\n",
        "                    break\n",
        "\n",
        "!runpodctl stop pod {pod_id}\n",
        "!runpodctl remove pod {pod_id}"
      ],
      "metadata": {
        "id": "FUYn-suBLNSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Q&A zu CSV\n",
        "import csv\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Function to read questions and answers from a text file and write to a CSV file\n",
        "def q_and_a_to_csv(input_file_path, output_file_path):\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", use_fast=True)\n",
        "\n",
        "    max_tokens = 0  # Variable to keep track of the maximum number of tokens in a line\n",
        "\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    # Initialize an empty list to hold (prompt, completion) pairs\n",
        "    prompts_and_completions = []\n",
        "\n",
        "    # Remove empty lines and strip leading/trailing whitespaces\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Loop through lines two at a time to get question and answer pairs\n",
        "    for i in range(0, len(lines), 2):\n",
        "        prompt = lines[i]\n",
        "        completion = lines[i + 1]\n",
        "\n",
        "        # Tokenize the lines and update max_tokens if necessary\n",
        "        prompt_tokens = tokenizer.tokenize(prompt)\n",
        "        completion_tokens = tokenizer.tokenize(completion)\n",
        "\n",
        "        max_tokens = max(max_tokens, len(prompt_tokens) + len(completion_tokens))\n",
        "\n",
        "        prompts_and_completions.append((prompt, completion))\n",
        "\n",
        "    # Write the list of (prompt, completion) pairs to a CSV file\n",
        "    with open(output_file_path, 'w', newline='') as output_file:\n",
        "        csv_writer = csv.writer(output_file, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
        "        csv_writer.writerow(['prompt', 'completion'])  # Write the header row\n",
        "        csv_writer.writerows(prompts_and_completions)\n",
        "\n",
        "    print(f\"The maximum number of tokens (prompt + completion) in a row of {output_file_path} is {max_tokens}\")\n",
        "\n",
        "# Specify the paths for the input and output files\n",
        "train_input_file_path = 'data/train.txt'\n",
        "train_output_file_path = 'data/train.csv'\n",
        "\n",
        "# Run the function to convert train.txt to train.csv\n",
        "q_and_a_to_csv(train_input_file_path, train_output_file_path)\n",
        "\n",
        "# Check if test.txt exists, and if so, convert it to text.csv\n",
        "text_input_file_path = 'data/test.txt'\n",
        "text_output_file_path = 'data/test.csv'\n",
        "\n",
        "if os.path.exists(text_input_file_path):\n",
        "    q_and_a_to_csv(text_input_file_path, text_output_file_path)\n",
        "else:\n",
        "    print(\"test.txt does not exist, skipping its conversion.\")"
      ],
      "metadata": {
        "id": "Z83vRdN9La5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8usA7WMwVCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. CSV zu Huggingface?\n",
        "from huggingface_hub import HfApi, login\n",
        "import os\n",
        "\n",
        "def upload_to_hf_hub(repo_id):\n",
        "    # Initialize HfApi\n",
        "    api = HfApi()\n",
        "\n",
        "    # Define the files to upload\n",
        "    files_to_upload = [\"data/train.csv\", \"data/test.csv\", \"data/README.md\"]\n",
        "    uploaded_files = []\n",
        "\n",
        "    # Upload each file if it exists\n",
        "    for file_path in files_to_upload:\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"Uploading {file_path}...\")\n",
        "            api.upload_file(\n",
        "                path_or_fileobj=file_path,\n",
        "                path_in_repo=os.path.basename(file_path),  # Only the filename, not the path\n",
        "                repo_id=repo_id,\n",
        "                repo_type=\"dataset\",\n",
        "            )\n",
        "            print(f\"Uploaded {file_path}.\")\n",
        "            uploaded_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"{file_path} does not exist, skipping.\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\nSummary:\")\n",
        "    if uploaded_files:\n",
        "        print(\"Uploaded files:\")\n",
        "        for file in uploaded_files:\n",
        "            print(f\"- {file}\")\n",
        "    else:\n",
        "        print(\"No files were uploaded.\")\n",
        "\n",
        "def main():\n",
        "    # Login\n",
        "    print(\"Logging in to Hugging Face account...\")\n",
        "    login()\n",
        "\n",
        "    # Get repo path from the user\n",
        "    repo_id = input(\"Enter the path to the Hugging Face dataset repo (e.g. username/repo_name): \")\n",
        "\n",
        "    # Upload files to the repo\n",
        "    upload_to_hf_hub(repo_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q_TTNDi8LliB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}